<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <meta name="author" content="Sebastian Raschka">
    <meta property="og:title" content="
      Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)
    ">
    <meta property="og:description" content="
        In the rapidly evolving field of AI, using large language models in an efficient and effective manner is becoming more and more important. In this article, y...
      ">
    <meta property="og:url"
        content="https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html" />
    <meta property="og:site_name" content="Sebastian Raschka, PhD" />
    <meta property="og:locale" content="en_US" />
    <meta name="twitter:site" content="@rasbt" />
    <meta name="twitter:description"
        content="In the rapidly evolving field of AI, using large language models in an efficient and effective manner is becoming more and more important. In this article, y..." />

    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2023-04-26 08:00:00 +0000" />
    
    <meta property="article:tag" content="Deep" />
    
    <meta property="article:tag" content="Learning," />
    
    <meta property="article:tag" content="Machine" />
    
    <meta property="article:tag" content="Data" />
    
    <meta property="article:tag" content="Science," />
    
    <meta property="article:tag" content="LLM" />
    
    <meta name="twitter:creator" content="@rasbt" />
    <meta name="twitter:title" content="Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)" />
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://sebastianraschka.com/images/blog/2023/llm-finetuning-llama-adapter/hero.jpg">
    


    <title>Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)</title>
    <meta name="description" content="My name is Sebastian, and I am a machine learning and AI researcher with a  strong passion for education. As Lead AI Educator at Grid.ai, I am excited  about making AI & deep learning more accessible and teaching people how to  utilize AI & deep learning at scale. I am also an Assistant Professor of Statistics  at the University of Wisconsin-Madison  and author of the bestselling book Python Machine Learning.
">
    <link rel="stylesheet" href=" /css/main.css">
    <link rel="stylesheet" href=" /css/signup-form.css">
    <link rel="stylesheet" href=" /css/fork-awesome.min.css">
    <!--<link rel="stylesheet" href=" /css/academicons.min.css">-->
    <meta property='og:title' content="Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)">
<meta property="og:type" content="article">
<meta property="og:url" content="https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html">


  <meta property="og:image" content="">


<meta property="og:description" content="In the rapidly evolving field of AI, using large language models in an efficient and effective manner is becoming more and more important. In this article, y...">
<meta property="og:site_name" content="Sebastian Raschka, PhD">
<meta property="og:locale" content="en_US">

    <meta property="article:published_time" content="2023-04-26T08:00:00+00:00">
    <meta property="article:author" content="">
    
        <meta property="og:see_also" content="https://sebastianraschka.com/blog/2023/llm-mixed-precision.html">
    
        <meta property="og:see_also" content="https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html">
    
        <meta property="og:see_also" content="https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html">
    


<meta property="fb:admins" content="">
<meta property="fb:app_id" content="">

    <link rel="canonical" href="https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html">


    <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicons/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/images/favicons/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#ffc40d">
    <meta name="theme-color" content="#ffffff">
</head>

  <body>
    <img src="/images/logos/ahead-of-ai-icon.png" alt="Ahead of AI logo" style="display: none;"> 

<header class="site-header">
  
      <div class="site-title" style="text-decoration: none; margin-top: 2em;">
        <a href="/"><span style="color:black">Sebastian</span> <span style="color:#c5050c">Raschka</span> </a>
        <a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20" style="padding-left:20px;" alt="Twitter icon"></a>
        <a href="https://www.linkedin.com/in/sebastianraschka/"><img src="/images/logos/linkedin-bw.jpg" height="20" style="padding-left:5px;" alt="LinkedIn Icon"></a>
      </div>



     <!--  <div style="width:100%;height:50;float:left;margin-bottom:10px;">
        &nbsp;<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-bw.jpg" height="20"></a>-->
        <!-- &nbsp;<a href="https://www.buymeacoffee.com/rasbt"><img src="/images/logos/coffee-bw.jpg" height="20"></a>-->
        <!--&nbsp;<a href="https://mastodon.social/@SebRaschka"><img src="/images/logos/mastodon-bw.jpg" height="20"></a>-->
        
     <!-- </div>-->
  

  <div class="wrapper">
  
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>



      
      <div class="trigger">



        <!--<script type="text/javascript">
          var total_images = 2;
          var random_number = Math.floor((Math.random()*total_images));
          var random_img = new Array();
          random_img[0] = '<a href="https://twitter.com/rasbt"><img src="/images/logos/twitter-1.jpg" height="20"></a>';
          random_img[1] = '<a href="https://linkedin.com/in/sebastianraschka"><img src="/images/logos/linkedin-1.jpg" height="20"></a>';
          document.write(random_img[random_number]);
          </script>-->

        <span style="padding-left:0px;margin-left:0px;"></span>
        <a class="page-link" href="https://magazine.sebastianraschka.com"><span style="color:#c5050c;"><img src="/images/logos/ahead-of-ai-icon.png" alt="Ahead of AI Logo" height="20"> AI Magazine</span></a>
        <!--<a class="page-link" href="/blog/index.html">Blog</a>-->
        <a class="page-link" href="/books">Books</a>
        <!--<a class="page-link" href="/newsletter">AI Newsletter</a>-->
        <a class="page-link" href="/teaching">Courses</a>
        <a class="page-link" href="/publications">Research</a>
        <a class="page-link" href="/elsewhere">Talks</a>
        <a class="page-link" href="/contact">Contact</a>
        <a class="page-link" href="/resources">Resources</a>
        

      </div>  


    </nav>


  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>




<div class="post">
  <header class="post-header">



    <div class="rss">

     
     &nbsp;&nbsp;<a href="/rss_feed.xml"><span><i class="fa fa-rss fa-2x"></i><br>RSS</a></span>

     

</div>


    <h1 class="post-title">Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)</h1>
    <h2 class="post-title"></h2>






    <div style='height: 15px;'></div>
    <p class="post-meta">Apr 26, 2023 <br>by Sebastian Raschka</p>
  </header>

  <article class="post-content">
    <h2 id="key-takeaways">
        
        
          Key Takeaways <a href="#key-takeaways">#</a>
        
        
      </h2>
    

<p>In the rapidly evolving field of AI, using large language models in an efficient and effective manner is becoming more and more important. In this article, you will learn how to tune an LLM with Low-Rank Adaptation (LoRA) in computationally efficient manner!</p>
      <h2 id="why-finetuning">
        
        
          Why Finetuning? <a href="#why-finetuning">#</a>
        
        
      </h2>
    

<p>Pretrained large language models are often referred to as foundation models for a good reason: they perform well on various tasks, and we can use them as a foundation for finetuning on a target task. As discussed in my previous article (<a href="https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html">Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters</a>), we discussed finetuning allows us to adapt a model to a target domain and target task. Still, it can be computationally very costly – the larger the model, the more expensive it is to update its layers.</p>

<p>As an alternative to updating all layers, parameter-efficient methods such as prefix tuning and adapters have been developed – for a detailed review, please see my <a href="https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html">previous post</a>. Now, there is one more popular parameter-efficient finetuning technique: <a href="https://arxiv.org/abs/2106.09685">Low-rank adaptation (LoRA) by Hu et al</a>. What is LoRA? How does it work? And how does it compare to the other popular finetuning approaches? Let’s answer all these questions in this article!</p>
      <h2 id="the-idea-behind-low-rank-adaptation">
        
        
          The Idea Behind Low-Rank Adaptation <a href="#the-idea-behind-low-rank-adaptation">#</a>
        
        
      </h2>
    

<p>The parameter-efficient <em>Low-rank adaptation</em> finetuning technique is, in a nutshell, an implicit low-rank transformation technique for large model weight matrices. So what is a low-rank transformation?</p>

<p>The overall idea and concept are related to principal component analysis (PCA) and singular vector decomposition (SVD), where we approximate a high-dimensional matrix or dataset using a lower-dimensional representation. In other words, we try to find a (linear) combination of a small number of dimensions in the original feature space (or matrix) that can capture most of the information in the dataset.</p>

<p><img src="/images/blog/2023/llm-finetuning-lora/pca.png" alt="pca" class="center-image image-90" /></p>
      <h2 id="making-weight-updates-more-efficient">
        
        
          Making Weight Updates More Efficient <a href="#making-weight-updates-more-efficient">#</a>
        
        
      </h2>
    

<p>Building on this idea outlined above, the paper <a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a> proposes to decompose the weight changes, <em>ΔW</em>, into a lower-rank representation. (To be technically correct, LoRA does not decompose the matrices directly, but it learns the decomposed matrices via backpropagation — this is a nitpicky detail that will make sense later).</p>

<p>Before we take a closer look at LoRA, let’s briefly explain the training procedure during regular finetuning. So, what are the weight changes <em>ΔW</em>? Suppose <em>W</em> represents the weight matrix in a given neural network layer. Then, using regular backpropagation, we can obtain the weight update <em>ΔW</em>, which is typically calculated as a negative gradient of the loss times the learning rate:</p>

<p>\(\Delta W = \alpha ( -\nabla L_W)\).</p>

<p>Then, when we have <em>ΔW</em>, we can update the original weights as follows: \(W' = W + \Delta W\). This is illustrated in the figure below (bias vectors are omitted for simplicity):</p>

<p>Alternatively, we can keep the weight update matrix separate and compute the outputs as follows: \(h = W x + \Delta W x,\)</p>

<p><img src="/images/blog/2023/llm-finetuning-lora/regular-finetuning.png" alt="regular-finetuning" class="center-image image-80" /></p>

<p>where \(x\) represents the inputs, as illustrated below.</p>

<p><img src="/images/blog/2023/llm-finetuning-lora/regular-finetuning-alt.png" alt="regular-finetuning-alt" class="center-image image-50" /></p>

<p>Why would we do this? For now, this alternative formulation serves a pedagogical goal to illustrate LoRA, but we will come back to it.</p>

<p>So, when we train fully connected (i.e., “dense”) layers in a neural network, as shown above, the weight matrices usually have full rank, which is a technical term meaning that a matrix does not have any linearly dependent (i.e., “redundant”) rows or columns. In contrast, to full rank, low rank means that the matrix has redundant rows or columns.</p>

<p>So, while the weights of a pretrained model have full rank on the pretrained tasks, the LoRA authors point out that pretrained large language models have a low “intrinsic dimension” when they are adapted to a new task, according to <a href="https://arxiv.org/abs/2012.13255">Aghajanyan et al.</a> (2020).</p>

<p>A low intrinsic dimension means the data can be effectively represented or approximated by a lower-dimensional space while retaining most of its essential information or structure. In other words, this means we can decompose the new weight matrix for the adapted task into lower-dimensional (smaller) matrices without losing too much important information.</p>

<p>For example, suppose \(\Delta W\) is the weight update for a weight matrix \(W \in \mathbb{R}^{A \times B}\). Then, we can decompose the weight update matrix into two smaller matrices: \(\Delta W = W_A W_B\), where \(W_A \in \mathbb{R}^{A \times r}\) and \(W_A \in \mathbb{R}^{r \times B}.\) Here, we keep the original weight \(W\) frozen and only train the new matrices \(W_A\) and \(W_B\). This, in a nutshell, is the LoRA method, which is illustrated in the figure below.</p>

<p><img src="/images/blog/2023/llm-finetuning-lora/lora-weights.png" alt="lora-weights" class="center-image image-50" /></p>
      <h3 id="choosing-the-rank">
        
        
          Choosing the rank <a href="#choosing-the-rank">#</a>
        
        
      </h3>
    

<p>Note that \(r\), in the figure above, is a hyperparameter here that we can use to specify the rank of the low-rank matrices used for adaptation. A smaller \(r\) leads to a simpler low-rank matrix, which results in fewer parameters to learn during adaptation. This can lead to faster training and potentially reduced computational requirements. However, with a smaller \(r\), the capacity of the low-rank matrix to capture task-specific information decreases. This may result in lower adaptation quality, and the model might not perform as well on the new task compared to a higher \(r\). In summary, choosing a smaller \(r\) in LoRA has a trade-off between model complexity, adaptation capacity, and the risk of underfitting or overfitting. It’s thus important to experiment with different \(r\) values to find the right balance to achieve the desired performance on the new task.</p>
      <h3 id="implementing-lora">
        
        
          Implementing LoRA <a href="#implementing-lora">#</a>
        
        
      </h3>
    

<p>The implementation of LoRA is relatively straight-forward. We can think of it as a modified forward pass for the fully connected layers in an LLM. In pseudo-code, this looks like as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">768</span>  <span class="c1"># e.g., the hidden size of the pre-trained model
</span><span class="n">output_dim</span> <span class="o">=</span> <span class="mi">768</span>  <span class="c1"># e.g., the output size of the layer
</span><span class="n">rank</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># The rank 'r' for the low-rank adaptation
</span>
<span class="n">W</span> <span class="o">=</span> <span class="p">...</span> <span class="c1"># from pretrained network with shape input_dim x output_dim
</span>
<span class="n">W_A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span> <span class="c1"># LoRA weight A
</span><span class="n">W_B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span> <span class="c1"># LoRA weight B
</span>
<span class="c1"># Initialization of LoRA weights
</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">W_A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">W_B</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">regular_forward_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span>
<span class="k">return</span> <span class="n">h</span>

<span class="k">def</span> <span class="nf">lora_forward_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">W_A</span><span class="p">,</span> <span class="n">W_B</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span>  <span class="c1"># regular matrix multiplication
</span>    <span class="n">h</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">@</span> <span class="p">(</span><span class="n">W_A</span> <span class="o">@</span> <span class="n">W_B</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span> <span class="c1"># use scaled LoRA weights
</span><span class="k">return</span> <span class="n">h</span>
</code></pre></div></div>

<p>In the pseudo-code above, <code class="language-plaintext highlighter-rouge">alpha</code> is a scaling factor that adjusts the magnitude of the combined result (original model output plus low-rank adaptation). This balances the pretrained model’s knowledge and the new task-specific adaptation — by default, <code class="language-plaintext highlighter-rouge">alpha</code> is usually set to 1. Also note that while \(W_A\) is initialized to small random weights, \(W_B\) is initialized to 0 so that</p>

<p>\(\Delta W = W_A W_B = 0\) at the beginning of the training, meaning we begin the training with the original weights.</p>
      <h3 id="parameter-efficiency">
        
        
          Parameter efficiency <a href="#parameter-efficiency">#</a>
        
        
      </h3>
    

<p>Now, let’s address the big elephant in the room: how is this parameter efficient if we introduce new weight matrices? The new matrices \(W_A\) and \(W_B\) can be very small. For example, suppose \(A=100\) and \(B=500\), then the size of \(\Delta W\) is \(100 \times 500 = 50,000\). Now, if we decompose this into two smaller matrices \(W_A \in \mathbb{R}^{100 \times 5}\) and \(W_B \in \mathbb{R}^{5 \times 500}\) , we only have \(5\times 100 + 5 \times 500 = 3,000\) parameters in total.</p>
      <h3 id="reducing-inference-overhead">
        
        
          Reducing inference overhead <a href="#reducing-inference-overhead">#</a>
        
        
      </h3>
    

<p>Note that in practice, if we keep the original weights \(W\) and the matrices \(W_A\) and \(W_B\) separate after training as shown above, we will incur a small efficiency penalty during inference as this introduces an additional computation step. Instead, we can update the weights after training via \(W' = W + W_A W_B\), which is analogous to \(W' = W + \Delta W\) mentioned earlier.</p>

<p>However, there can be practical advantages in keeping the weight matrices \(W_A\) and \(W_B\) separate. For example, imagine we want to keep our pretrained model as a base model for various customers, and we want to create a finetuned LLM for each customer starting from the base model. In this case, we don’t need to store the full weight matrices \(W'\) for each customer, where storing all the weights \(W' = W + W_A W_B\) for a model can be very large for LLMs, since LLMs typically have billions to trillions of weight parameters. So instead, we can keep the original model \(W\) and only need to store the new lightweight matrices \(W_A\) and \(W_B\).</p>

<p>To illustrate this point with concrete numbers, a full 7B LLaMA checkpoint requires 23 GB of storage capacity, while the LoRA weights can be as small as 8 MB if we choose a rank of \(r=8\).</p>
      <h3 id="how-good-is-it-in-practice">
        
        
          How good is it in practice? <a href="#how-good-is-it-in-practice">#</a>
        
        
      </h3>
    

<p>How good is LoRA in practice, and how does it compare to full finetuning and other parameter-efficient approaches? According to the <a href="https://arxiv.org/abs/2106.09685">LoRA paper</a>, the modeling performance of models using LoRA performs slightly better than models using <a href="https://arxiv.org/abs/2110.07280">Adapters</a>, <a href="https://arxiv.org/abs/2104.08691">prompt tuning</a>, or <a href="https://arxiv.org/abs/2101.00190">prefix tuning</a> across several task-specific benchmarks. Often, LoRA performs even better than finetuning all layers, as shown in the annotated table from the LoRA paper below. (ROUGE is a metric for evaluating language translation performance, I explained it in more detail <a href="https://twitter.com/rasbt/status/1639625228622917632?s=20">here</a>.)</p>

<p><img src="/images/blog/2023/llm-finetuning-lora/lora-table-2.png" alt="lora-table-2" class="center-image image-70" /></p>

<p>Here, it’s worth noting that LoRA is orthogonal to the other finetuning methods, meaning it can also be combined with prefix tuning and adapters, for example.</p>
      <h2 id="lora--llama">
        
        
          LoRA &amp; LLaMA <a href="#lora--llama">#</a>
        
        
      </h2>
    

<p>Now, let’s work with an implementation of LoRA for finetuning Meta’s popular LLaMA model. Since this is already a long article, I will refrain from including the detailed code in this article itself, but I recommend checking out the <a href="https://github.com/Lightning-AI/lit-llama">Lit-LLaMA repository</a>, which is a simple, readable reimplementation of Meta’s popular LLaMA model.</p>

<p>Besides code for training and running LLaMA itself (with the original Meta LLaMA weights), it also contains code for finetuning LLaMA using <a href="https://github.com/Lightning-AI/lit-llama/blob/main/finetune_adapter.py">LLaMA-Adapter</a> and <a href="https://github.com/Lightning-AI/lit-llama/blob/main/finetune_lora.py">LoRA</a>.</p>

<p>To get started, I recommend the following <em>How-To</em> files:</p>

<ol>
  <li>Downloading pretrained weights [ <a href="https://github.com/Lightning-AI/lit-llama/blob/main/howto/download_weights.md">download_weights.md</a> ]</li>
  <li>Finetuning with LoRA [ <a href="https://github.com/Lightning-AI/lit-llama/blob/main/howto/finetune_lora.md">finetune_lora.md</a> ]</li>
  <li>Finetuning with Adapter [ <a href="https://github.com/Lightning-AI/lit-llama/blob/main/howto/finetune_adapter.md">finetune_adapter.md</a> ] (optional, for comparison studies)</li>
</ol>

<p>In the next section, we will compare the 7B LLaMA base model with the 7B LLaMA base finetuned using LoRA and LLaMA-Adapter. (Note that this requires a GPU with at least 24 Gb RAM). (For more details on the LLaMA-Adapter method, please see my <a href="https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html">previous article</a>)</p>
      <h2 id="lora-llama-computational-performance-benchmarks">
        
        
          LoRA-LLaMA Computational Performance Benchmarks <a href="#lora-llama-computational-performance-benchmarks">#</a>
        
        
      </h2>
    

<p>In this section, we will compare the computational performance of the LLaMA 7B base model with the base model finetuned using LoRA and LLaMA-Adapter.</p>

<p>The finetuning dataset is the Alpaca 52k instruction dataset described <a href="https://github.com/tatsu-lab/stanford_alpaca#data-release">here</a>, which has the following structure:</p>

<p><img src="/images/blog/2023/llm-finetuning-lora/alpaca-instruct.png" alt="alpaca-instruct" class="center-image image-80" /></p>

<p>The dataset itself was generated following the method described in the <a href="https://arxiv.org/abs/2212.10560">Self-Instruct paper</a> and consists of 49,759 training examples and 2000 validation examples. The Self-Instruct procedure can be summarized in 4 steps:</p>

<p>How does this work? In a nutshell, it’s a 4-step process</p>

<ol>
  <li>Seed task pool with a set of human-written instructions (175 in this case) and sample instructions</li>
  <li>Use a pretrained LLM (like GPT-3) to determine the task category</li>
  <li>Given the new instruction, let a pretrained LLM generate the response</li>
  <li>Collect, prune, and filter the responses before adding it to the task pool</li>
</ol>

<p><img src="/images/blog/2023/llm-finetuning-lora/self-instruct.png" alt="self-instruct" class="center-image image-100" /></p>

<p>Note that the Alpaca 52k dataset was collected using the automated self-instruct procedure above. However, you may also use (or compare it with) an alternative dataset. For example, an interesting candidate is the recently released open-source <a href="https://github.com/databrickslabs/dolly/tree/master/data">databricks-dolly-15k</a> dataset that contains ~15k instruction/response finetuning records written by Databricks employees. The Lit-LLaMA repository contains a dataset preparation script in case you want to use this Dolly 15k dataset instead of the Alpaca 52k dataset.</p>

<p>Given the following hyperparameter settings (block size, batch size, and LoRA r) both Adapter and LoRA can finetune the 7B parameter LLaMA base model on a single GPU with 24 Gb RAM using bfloat-16 mixed precision training.</p>
      <h4 id="lora">
        
        
          LoRA <a href="#lora">#</a>
        
        
      </h4>
    

<div class="language-jsx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">learning_rate</span> <span class="o">=</span> <span class="mi">3</span><span class="nx">e</span><span class="o">-</span><span class="mi">4</span>
<span class="nx">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="nx">micro_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="nx">gradient_accumulation_steps</span> <span class="o">=</span> <span class="nx">batch_size</span> <span class="c1">// micro_batch_size</span>
<span class="nx">epoch_size</span> <span class="o">=</span> <span class="mi">50000</span>  <span class="err">#</span> <span class="nx">train</span> <span class="nx">dataset</span> <span class="nx">size</span>
<span class="nx">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="nx">max_iters</span> <span class="o">=</span> <span class="nx">num_epochs</span> <span class="o">*</span> <span class="nx">epoch_size</span> <span class="c1">// micro_batch_size // devices</span>
<span class="nx">weight_decay</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="nx">block_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="nx">lora_r</span> <span class="o">=</span> <span class="mi">8</span>
<span class="nx">lora_alpha</span> <span class="o">=</span> <span class="mi">16</span>
<span class="nx">lora_dropout</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="nx">warmup_steps</span> <span class="o">=</span> <span class="mi">100</span>
</code></pre></div></div>
      <h4 id="llama-adapter">
        
        
          LLaMA Adapter <a href="#llama-adapter">#</a>
        
        
      </h4>
    

<div class="language-jsx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">learning_rate</span> <span class="o">=</span> <span class="mi">9</span><span class="nx">e</span><span class="o">-</span><span class="mi">3</span>
<span class="nx">batch_size</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">/</span> <span class="nx">devices</span>
<span class="nx">micro_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="nx">gradient_accumulation_steps</span> <span class="o">=</span> <span class="nx">batch_size</span> <span class="c1">// micro_batch_size</span>
<span class="nx">epoch_size</span> <span class="o">=</span> <span class="mi">50000</span>  <span class="err">#</span> <span class="nx">train</span> <span class="nx">dataset</span> <span class="nx">size</span>
<span class="nx">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="nx">max_iters</span> <span class="o">=</span> <span class="nx">num_epochs</span> <span class="o">*</span> <span class="nx">epoch_size</span> <span class="c1">// micro_batch_size // devices</span>
<span class="nx">weight_decay</span> <span class="o">=</span> <span class="mf">0.02</span>
<span class="nx">block_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="nx">warmup_steps</span> <span class="o">=</span> <span class="nx">epoch_size</span> <span class="o">*</span> <span class="mi">2</span> <span class="c1">// micro_batch_size // devices</span>
</code></pre></div></div>
      <h4 id="full-finetuning">
        
        
          Full finetuning <a href="#full-finetuning">#</a>
        
        
      </h4>
    

<div class="language-jsx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">learning_rate</span> <span class="o">=</span> <span class="mi">3</span><span class="nx">e</span><span class="o">-</span><span class="mi">5</span>
<span class="nx">batch_size</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">/</span> <span class="nx">devices</span>
<span class="nx">micro_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="nx">gradient_accumulation_steps</span> <span class="o">=</span> <span class="nx">batch_size</span> <span class="c1">// micro_batch_size</span>
<span class="nx">epoch_size</span> <span class="o">=</span> <span class="mi">50000</span>  <span class="err">#</span> <span class="nx">train</span> <span class="nx">dataset</span> <span class="nx">size</span>
<span class="nx">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="nx">max_iters</span> <span class="o">=</span> <span class="nx">num_epochs</span> <span class="o">*</span> <span class="nx">epoch_size</span> <span class="c1">// micro_batch_size // devices</span>
<span class="nx">weight_decay</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="nx">block_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="nx">warmup_steps</span> <span class="o">=</span> <span class="mi">100</span>
</code></pre></div></div>

<p>In case the code changes in the future, I am including the code (with hyperparameter settings) <a href="https://github.com/rasbt/low-rank-adaptation-blog">here on GitHub</a>.</p>

<p>Adapter used about 22 Gb and finished 62,400 iterations in 162 min on an A100. LoRA used 21 Gb of memory and finished in 192 min. In sum, Adapter and LoRA use approximately the same amount of RAM and have roughly the same training time based on the Lit-LLaMA implementations. (Note that this is on a single GPU, but if you have multiple GPUs, just change the <code class="language-plaintext highlighter-rouge">devices</code> parameter to &gt; 1 to take advantage of additional speedups!)</p>

<p>For comparison, full finetuning (LLaMA 7B consists of 32 transformer blocks and 3 fully connected output layers) required at least 2 GPUs with at least 30 Gb and fully sharded training to distribute the weights. Alternatively, you can use 4 GPUs with a maximum memory usage of 22 Gb per GPU. The training on 4 GPUs and the training took 1956 min. This would be at least 6,000 min on a single GPU, which would be 30-40x more expensive than the parameter-efficient LLaMA-Adapter or LoRA alternatives.</p>

<p>Next, let’s look at the model outputs after applying the different finetuning strategies.</p>
      <h2 id="evaluating-modeling-performance">
        
        
          Evaluating Modeling Performance <a href="#evaluating-modeling-performance">#</a>
        
        
      </h2>
    

<p>There are several metrics for evaluating the text generated by LLMs. For example, perplexity, BLEU, and ROUGE scores are some of the most common evaluation metrics used in natural language processing to assess the performance of LLMs across various tasks. However, all of these metrics have substantial shortcomings, and human evaluations remain the gold standard – the downside of human evaluations is that they are expensive to create and hard to automate. Since this is already a very long article, I will refrain from a detailed discussion of model evaluation approaches and will defer this to a separate article in the future. In this future article, we will be looking at different Q&amp;A datasets (including <a href="https://paperswithcode.com/dataset/wikitext-2"><code class="language-plaintext highlighter-rouge">wikitext</code></a>, <a href="https://paperswithcode.com/dataset/c4"><code class="language-plaintext highlighter-rouge">c4</code></a>, and <a href="https://www.kaggle.com/datasets/bjoernjostein/ptbxl-electrocardiography-database"><code class="language-plaintext highlighter-rouge">ptb</code></a>, as well as the above-mentioned metrics.) So, for simplicity, we will only be focusing on a qualitative model comparison below, looking at some outputs generated by the various models.</p>
      <h2 id="a-qualitative-comparison">
        
        
          A Qualitative Comparison <a href="#a-qualitative-comparison">#</a>
        
        
      </h2>
    

<p>To compare the models qualitatively, we will be using the prompt <code class="language-plaintext highlighter-rouge">"Please explain how weight decay works"</code></p>

<p><strong>Base model</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>python generate.py <span class="nt">--checkpoint_path</span> checkpoints/lit-llama/7B/lit-llama.pth
</code></pre></div></div>

<blockquote>
  <p><em>Please explain how weight decay works…and why it sometimes seems to work and sometimes doesn’t!</em></p>

  <p><em>In the simplest terms, weight decay removes features from the model (as a function of how important they are). It is also known as regularization.</em></p>

  <p><em>Regularization has</em></p>

  <p>Time for inference 1: 4.18 sec total, 11.95 tokens/sec</p>

  <p>Memory used: 13.54 GB</p>
</blockquote>

<p>As we can see, the model begins to continue the prompt by appending “…and why it sometimes seems to work and sometimes doesn’t!” That’s because the base model was only trained on a next-word prediction task and wasn’t specifically trained to follow instructions. Nonetheless, it attempts the explanation further below. The explanation makes somewhat correct statements about regularization in general, but it doesn’t really capture the key mechanics of weight decay. (To be fair, it should be mentioned that the training set input lengths were capped at 256 tokens for efficiency reasons.)</p>
      <h4 id="llama-adapter-1">
        
        
          LLaMA-Adapter <a href="#llama-adapter-1">#</a>
        
        
      </h4>
    

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>python generate_adapter.py <span class="nt">--adapter_path</span> out/adapter/alpaca/lit-llama-adapter-finetuned.pth <span class="nt">--pretrained_path</span> checkpoints/lit-llama/7B/lit-llama.pth
</code></pre></div></div>

<blockquote>
  <p><em>Weight decay is a way to penalize terms for their word-count. By choosing sentences with fewer words, a model is more likely to select those sentences, which may provide important information.</em></p>

  <p>Time for inference: 7.04 sec total, 14.21 tokens/sec</p>

  <p>Memory used: 13.56 GB</p>
</blockquote>

<p>The explanation is grammatically sound, but it is also not necessarily what we are looking for.</p>
      <h4 id="lora-1">
        
        
          LoRA <a href="#lora-1">#</a>
        
        
      </h4>
    

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>python generate_lora.py <span class="nt">--lora_path</span> out/lora/alpaca/lit-llama-lora-finetuned.pth <span class="nt">--pretrained_path</span> checkpoints/lit-llama/7B/lit-llama.pth
</code></pre></div></div>

<blockquote>
  <p><em>Weight decay is an algorithm parameter used in machine learning models to reduce overfitting. It is a process where the weights of the model are adjusted in order to decrease the influence of less-important features in the model and prevent overfitting. Weight decay is used by adding a small amount of noise to the weights to reduce the impact of the model on the training data. This process encourages the model to focus more on the features that are more important to the predictive task.</em></p>

  <p>Time for inference: 3.25 sec total, 30.76 tokens/sec</p>

  <p>Memory used: 27.65 GB</p>
</blockquote>

<p>We are getting closer; the explanation does make sense when we think of generalized linear models where we force the model to learn smaller weight parameters that are multiplied with the input features. In neural networks, this would typically be applied to all weight parameters in the model.</p>

<p>Note that the LoRA approach above currently uses the most memory. However, we can reduce this memory usage by merging the LoRA weights with the pretrained model weights, as described earlier.</p>

<p>This qualitative overview is only a thin slice of the capabilities of each of these models since evaluating LLMs is a big topic in itself. We will revisit this topic in a more detailed article in the future. But as a takeaway here, LoRA can be used to finetuning an LLM on an instruction dataset in a relatively cost-effective manner.</p>
      <h2 id="conclusion">
        
        
          Conclusion <a href="#conclusion">#</a>
        
        
      </h2>
    

<p>In this article, we discussed low-rank adaptation (LoRA), a parameter-efficient alternative to full finetuning. We saw that finetuning a relatively large model such as LLaMA can be done in a few hours on a single GPU using LoRA, which makes it particularly attractive to people who don’t want to spend thousands of dollars on GPU resources. What’s particularly nice about LoRA is that we can optionally merge the new LoRA weight matrices with the original, pretrained weights, such that we don’t incur additional overheads or complexity during inference.</p>

<p>As more and more open-source alternatives to ChatGPT or GPT-4 emerge, finetuning and customizing these LLMs on specific target datasets or targets will become more and more attractive across various research fields and industries. And parameter-efficient finetuning techniques such as LoRA make finetuning more resource-efficient and accessible.</p>

<p>Parameter-efficient finetuning techniques such as LoRA and LLaMA-Adapter are provided in the <a href="https://github.com/Lightning-AI/lit-llama">Lit-LLaMA repository</a>. We are always happy about contributions and suggestions if you have ideas for extensions or alternative techniques. Please don’t hesitate to reach out to us via <a href="https://github.com/Lightning-AI/lit-llama">GitHub</a> or <a href="https://discord.com/invite/XncpTy7DSt">Discord</a>.</p>

<p><strong>Acknowledgements</strong></p>

<p>I want to thank Luca Antiga and Adrian Waelchli for the constructive feedback to improve the clarity of this article.</p>

    <br>
    <br>
    
    <hr>
    
    
    <br>
    
    If you liked this article, you can also find me on 
    <a href="https://twitter.com/rasbt">Twitter</a> and <a href="https://www.linkedin.com/in/sebastianraschka/">LinkedIn</a> where I share more content related to machine learning and AI.
    <br> 
    If you are looking for a way to support me and my work, consider purchasing <a href="/books">one of my books</a> or subscribing to the paid version of my free <a href="https://magazine.sebastianraschka.com/">AI newsletter</a>. If you find it valuable, I would really appreciate it if you could spread the word and recommend it to others.
    
    <p style="clear: both;"></p>
    <a href="https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-scikit-learn-ebook-dp-B09NW48MR1/dp/B09NW48MR1/"><img src="/images/books/pyml4-cover.jpg" height=100 style="padding-right:25px"></a>
    <a href="https://leanpub.com/machine-learning-q-and-ai/"><img src="/images/books/2023-ml-qai-cover.jpg"  height=100></a>
    <p style="clear: both;"></p>

  </article>


  <!--<strong>Have feedback on this post? I would love to hear it. Let me know and send me a <a href="https://twitter.com/intent/tweet?text=. @rasbt http://sebastianraschka.com/blog/2023/llm-finetuning-lora.html">tweet</a> or <a href="http://sebastianraschka.com/email.html">email</a>.</strong>-->

</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">



    <div class="footer-col-wrapper">


    <div class="footer-col  social-col">


      
      <a href="https://magazine.sebastianraschka.com"><span><i class="fa fa-rss fa-2x"></i></span> </a>
      

      
          <a href="/contact"><span><i class="fa fa-envelope fa-2x"></i></span> </a>
      

      
        <a href="https://twitter.com/rasbt"> <span><i class="fa fa-twitter fa-2x"></i></span> </a>
      

      
            <a href="https://youtube.com/c/SebastianRaschka"><span><i class="fa fa-youtube fa-2x"></i></span> </a>
      

      
            <a href="https://github.com/rasbt"><span><i class="fa fa-github fa-2x"></i> </span></a>
      

      
          <a href="https://scholar.google.com/citations?user=X4RCC0IAAAAJ&hl=enrasbt"><span><i class="fa fa-google fa-2x"></i> </span></a>
      

      
          <a href="https://linkedin.com/in/sebastianraschka"><span><i class="fa fa-linkedin fa-2x"></i> </span></a>
      


  </div>

    <div class="footer-col  copyright-col">
      <p>&copy; 2013-2023 Sebastian Raschka</p>
    </div>



  </div>
</div>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BYQXBRPK81"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BYQXBRPK81');
</script>
</footer>
</div>
  

  <script src="/js/anchor.min.js" type="text/javascript"></script>
  <script>
    var selector = 'h2, h3, h4, h5, h6';
    /*
    anchors.options = {
      icon: '#',
      visible: 'always',
      placement: 'left',
      class: 'bb-anchor'
    }
    */
    anchors.add(selector);
  </script>

</body>
</html>
